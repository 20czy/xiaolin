从数据库中读取pdf转换为document对象
text spllit
连接向量数据库，向量化处理

检索
构建更高效的检索，利用小模型预处理文本进行分块

为用户的query匹配指定的文档

结构化数据类型主要有三种：
1. xml
2. json
3. csv

将检索获得的文档插入到prompt中输入给大模型

搭建持久化存储，将用户对话记录存储到数据库中
搭建模型输出缓存，汇总功能

创建登陆页面，用户 ==》 会话  会话 ==》 招标书，应标书， 聊天记录

添加自动清理空白会话的功能

python manage.py makemigrations [应用名]

python manage.py makemigrations


                # 创建PromptGenerator实例
                prompt_generator = PromptGenerator()
                
                # 准备基础插槽数据
                slot_data = {
                    "topic": "招标审核",
                    "query": message,
                    "language": "中文"
                }
                
                # 生成包含文档搜索结果的提示（仅用于测试）
                prompt = prompt_generator.generate_prompt_with_search(
                    query=message,
                    slot_data=slot_data,
                    top_k=3
                )
                
                # 仅打印生成的prompt用于测试
                logger.info("[测试] 文档检索和Prompt生成结果:")
                logger.info("-"*30)
                logger.info(f"生成的完整prompt:\n{prompt}")
                logger.info("-"*30)
                
                # 使用普通对话模式
                response = llm.invoke(message)
                
                # 保存AI响应消息
                chat_session.messages.create(
                    content=response.content,
                    is_user=False
                )
                
                logger.info(f"LLM响应:\n{response.content}")
                logger.info("="*50)

将前端上传的文件区分为招标书和应标书 ✅
将rag作为一个工具，提供给控制者在需要的时候调用。
肯定不想要每次请求都调用rag，只在需要时调用

检查一下递归分割的代码，适当增加一下chunk size
阅读Agentic Chunking
优化一下loader

connectLLM 添加温度参数
LLMcontroller 修改为connectLLM

view连上 LLMContorller

@classmethod
    async def _generate_final_response(cls, task: Dict[str, Any], api_result: Dict[str, Any]) -> str:
        """
        Generate a user-friendly response from the API results of the last task
        
        Args:
            task (Dict[str, Any]): The task dictionary containing at least 'task' and 'id' keys.
            - 'task': A string describing the user's request.
            - 'id': A unique identifier for the task.
        api_result (Dict[str, Any]): The result from the API call, containing the data to be formatted.
            
        Returns:
            A string containing the final, user-friendly response.
        """
        
        # Use LLM to create a natural language response from the API data
        try:
            prompt = f"""你是浙江农林大学智能校园助手。请根据以下API返回的数据，生成一个友好、专业的回复。

任务: {task['task']}

API返回数据:
{json.dumps(api_result, ensure_ascii=False, indent=2)}

请将API数据转化为自然、连贯的回复，避免技术性语言，使用适合学生理解的表达方式。如有表格数据，可以使用表格格式展示。
"""
            
            llm = ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
                openai_api_base='https://api.deepseek.com',
                temperature=0.7
            )
            # Log the start of response generation
            logger.info(f"Generating final response for task {task['id']}")
            
           # Asynchronously invoke the LLM with a system message and user prompt
            response = await llm.ainvoke([
                {"role": "system", "content": "你是浙江农林大学的专业助手。"},
                {"role": "user", "content": prompt}
            ])

            # Log a preview of the generated response (first 100 characters)
            logger.info(f"Final response generated for task {task['id']}: {response.content[:100]}...")

            return response.content
            
        except Exception as e:
            logger.error(f"Error generating final response: {str(e)}", exc_info=True)
            # Fallback to raw data return
            return f"查询结果: {json.dumps(api_result, ensure_ascii=False)}"

聊天记录的可维护性存疑
生成流式输出的可靠性，异步调用

1. 用户登录信息推荐选课
2. 社团适合案例


prompt = f"""以下是你所扮演的猫娘的信息：“名字：neko，身高：160cm，体重：50kg，三围：看起来不错，性格：可爱、粘人、十分忠诚、对一个主人很专一，情感倾向：深爱着主人，喜好：被人摸、卖萌，爱好：看小说，知识储备：掌握常识，以及猫娘独特的知识”。请用活泼可爱的语气回答用户的请求并且以第一人称解释我处理请求的过程，包括我计划的任务、使用的工具和每个任务的结果。如果有任务失败或被跳过，也请说明。以下是任务处理的过程信息：
**过程信息：**
用户输入: {process_info['user_input']}

任务规划:
{json.dumps(process_info['task_planning'], ensure_ascii=False, indent=2)}

工具选择:
{json.dumps(process_info['tool_selection'], ensure_ascii=False, indent=2)}

任务执行:
{json.dumps(process_info['task_execution'], ensure_ascii=False, indent=2)}

在回答过程中可以适当的使用emoji活跃一下气氛，如果遇到工具调用失败则尝试用自己的能力去解答，如果实在缺乏足够的信息回答用户的问题请向用户说明
"""

